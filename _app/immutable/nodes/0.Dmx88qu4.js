import{a as G,o as j,j as M,g as p,P as $,R as B,S as ee,U as le,V as ae,W as ne,X as ie,e as D,c as I,d as E,a1 as pe,Y as Qe,a2 as De,a3 as ye,w as Te,T as Ke,Z as Xe,_ as S,b as U,h as F,u as m,a4 as we,a5 as A,r as ve,t as X,a6 as re,f as Z,k as w,a7 as Ie,l as oe,n as Y,a8 as Ze,$ as te,a0 as se,a9 as me,q as Ye,aa as qe,m as ue,ab as fe,ac as Je,ad as xe,B as $e,ae as et}from"../chunks/scheduler.BvdSgcxT.js";import{S as J,i as x,g as de,b as V,e as he,t as P,f as Re,c as K,a as R,m as O,d as Q}from"../chunks/index.DGxhJfay.js";import{g as ce,b as W,a as tt,c as st,d as lt,e as Le,f as at,i as nt,h as it,p as ot,j as ke,s as rt,k as Pe,l as Me,r as ct,m as ut,n as dt,o as ht}from"../chunks/index.Hge3QCFG.js";import{P as ft,H as pt}from"../chunks/HelpPopover.CAW2kZqb.js";import{w as mt}from"../chunks/index.CRtEE1wu.js";import"../chunks/paths.C9r5A9pZ.js";function vt(l){let e;const t=l[5].default,n=le(t,l,l[4],null);return{c(){n&&n.c()},l(s){n&&n.l(s)},m(s,a){n&&n.m(s,a),e=!0},p(s,a){n&&n.p&&(!e||a&16)&&ae(n,t,s,s[4],e?ie(t,s[4],a,null):ne(s[4]),null)},i(s){e||(P(n,s),e=!0)},o(s){V(n,s),e=!1},d(s){n&&n.d(s)}}}function gt(l){let e=l[0],t,n,s=l[0]&&be(l);return{c(){s&&s.c(),t=j()},l(a){s&&s.l(a),t=j()},m(a,i){s&&s.m(a,i),M(a,t,i),n=!0},p(a,i){a[0]?e?G(e,a[0])?(s.d(1),s=be(a),e=a[0],s.c(),s.m(t.parentNode,t)):s.p(a,i):(s=be(a),e=a[0],s.c(),s.m(t.parentNode,t)):e&&(s.d(1),s=null,e=a[0])},i(a){n||(P(s,a),n=!0)},o(a){V(s,a),n=!1},d(a){a&&p(t),s&&s.d(a)}}}function be(l){let e,t,n,s;const a=l[5].default,i=le(a,l,l[4],null);let u=[l[3]],o={};for(let r=0;r<u.length;r+=1)o=B(o,u[r]);return{c(){e=D(l[0]),i&&i.c(),this.h()},l(r){e=I(r,(l[0]||"null").toUpperCase(),{});var c=E(e);i&&i.l(c),c.forEach(p),this.h()},h(){pe(l[0])(e,o)},m(r,c){M(r,e,c),i&&i.m(e,null),t=!0,n||(s=Qe(l[2].call(null,e)),n=!0)},p(r,c){i&&i.p&&(!t||c&16)&&ae(i,a,r,r[4],t?ie(a,r[4],c,null):ne(r[4]),null),pe(r[0])(e,o=ce(u,[c&8&&r[3]]))},i(r){t||(P(i,r),t=!0)},o(r){V(i,r),t=!1},d(r){r&&p(e),i&&i.d(r),n=!1,s()}}}function kt(l){let e,t,n,s;const a=[gt,vt],i=[];function u(o,r){return o[1]?0:1}return e=u(l),t=i[e]=a[e](l),{c(){t.c(),n=j()},l(o){t.l(o),n=j()},m(o,r){i[e].m(o,r),M(o,n,r),s=!0},p(o,[r]){let c=e;e=u(o),e===c?i[e].p(o,r):(de(),V(i[c],1,1,()=>{i[c]=null}),he(),t=i[e],t?t.p(o,r):(t=i[e]=a[e](o),t.c()),P(t,1),t.m(n.parentNode,n))},i(o){s||(P(t),s=!0)},o(o){V(t),s=!1},d(o){o&&p(n),i[e].d(o)}}}function bt(l,e,t){const n=["tag","show","use"];let s=$(e,n),{$$slots:a={},$$scope:i}=e,{tag:u="div"}=e,{show:o}=e,{use:r=()=>{}}=e;return l.$$set=c=>{e=B(B({},e),ee(c)),t(3,s=$(e,n)),"tag"in c&&t(0,u=c.tag),"show"in c&&t(1,o=c.show),"use"in c&&t(2,r=c.use),"$$scope"in c&&t(4,i=c.$$scope)},[u,o,r,s,i,a]}class _t extends J{constructor(e){super(),x(this,e,bt,kt,G,{tag:0,show:1,use:2})}}function yt(l){let e,t,n;const s=l[5].default,a=le(s,l,l[4],null);let i=[l[1],{class:t=W(l[0],l[2].class)},{role:"group"}],u={};for(let o=0;o<i.length;o+=1)u=B(u,i[o]);return{c(){e=D("div"),a&&a.c(),this.h()},l(o){e=I(o,"DIV",{class:!0,role:!0});var r=E(e);a&&a.l(r),r.forEach(p),this.h()},h(){De(e,u)},m(o,r){M(o,e,r),a&&a.m(e,null),n=!0},p(o,[r]){a&&a.p&&(!n||r&16)&&ae(a,s,o,o[4],n?ie(s,o[4],r,null):ne(o[4]),null),De(e,u=ce(i,[r&2&&o[1],(!n||r&5&&t!==(t=W(o[0],o[2].class)))&&{class:t},{role:"group"}]))},i(o){n||(P(a,o),n=!0)},o(o){V(a,o),n=!1},d(o){o&&p(e),a&&a.d(o)}}}function wt(l,e,t){const n=["size","divClass"];let s=$(e,n),{$$slots:a={},$$scope:i}=e,{size:u="md"}=e,{divClass:o="inline-flex rounded-lg shadow-sm"}=e;return ye("group",{size:u}),l.$$set=r=>{t(2,e=B(B({},e),ee(r))),t(1,s=$(e,n)),"size"in r&&t(3,u=r.size),"divClass"in r&&t(0,o=r.divClass),"$$scope"in r&&t(4,i=r.$$scope)},e=ee(e),[o,s,e,u,i,a]}class Tt extends J{constructor(e){super(),x(this,e,wt,yt,G,{size:3,divClass:0})}}const Ct=l=>({}),Ve=l=>({}),Et=l=>({}),ze=l=>({});function Ae(l){let e,t;const n=l[16].header,s=le(n,l,l[19],ze);return{c(){e=D("div"),s&&s.c(),this.h()},l(a){e=I(a,"DIV",{class:!0});var i=E(e);s&&s.l(i),i.forEach(p),this.h()},h(){m(e,"class",l[3])},m(a,i){M(a,e,i),s&&s.m(e,null),t=!0},p(a,i){s&&s.p&&(!t||i&524288)&&ae(s,n,a,a[19],t?ie(n,a[19],i,Et):ne(a[19]),ze),(!t||i&8)&&m(e,"class",a[3])},i(a){t||(P(s,a),t=!0)},o(a){V(s,a),t=!1},d(a){a&&p(e),s&&s.d(a)}}}function He(l){let e,t;const n=l[16].footer,s=le(n,l,l[19],Ve);return{c(){e=D("div"),s&&s.c(),this.h()},l(a){e=I(a,"DIV",{class:!0});var i=E(e);s&&s.l(i),i.forEach(p),this.h()},h(){m(e,"class",l[1])},m(a,i){M(a,e,i),s&&s.m(e,null),t=!0},p(a,i){s&&s.p&&(!t||i&524288)&&ae(s,n,a,a[19],t?ie(n,a[19],i,Ct):ne(a[19]),Ve),(!t||i&2)&&m(e,"class",a[1])},i(a){t||(P(s,a),t=!0)},o(a){V(s,a),t=!1},d(a){a&&p(e),s&&s.d(a)}}}function Dt(l){let e,t,n,s,a,i=l[6].header&&Ae(l);const u=l[16].default,o=le(u,l,l[19],null);let r=l[6].footer&&He(l);return{c(){i&&i.c(),e=U(),t=D("ul"),o&&o.c(),n=U(),r&&r.c(),s=j(),this.h()},l(c){i&&i.l(c),e=F(c),t=I(c,"UL",{class:!0});var d=E(t);o&&o.l(d),d.forEach(p),n=F(c),r&&r.l(c),s=j(),this.h()},h(){m(t,"class",l[2])},m(c,d){i&&i.m(c,d),M(c,e,d),M(c,t,d),o&&o.m(t,null),M(c,n,d),r&&r.m(c,d),M(c,s,d),a=!0},p(c,d){c[6].header?i?(i.p(c,d),d&64&&P(i,1)):(i=Ae(c),i.c(),P(i,1),i.m(e.parentNode,e)):i&&(de(),V(i,1,1,()=>{i=null}),he()),o&&o.p&&(!a||d&524288)&&ae(o,u,c,c[19],a?ie(u,c[19],d,null):ne(c[19]),null),(!a||d&4)&&m(t,"class",c[2]),c[6].footer?r?(r.p(c,d),d&64&&P(r,1)):(r=He(c),r.c(),P(r,1),r.m(s.parentNode,s)):r&&(de(),V(r,1,1,()=>{r=null}),he())},i(c){a||(P(i),P(o,c),P(r),a=!0)},o(c){V(i),V(o,c),V(r),a=!1},d(c){c&&(p(e),p(t),p(n),p(s)),i&&i.d(c),o&&o.d(c),r&&r.d(c)}}}function It(l){let e,t,n;const s=[{activeContent:!0},l[5],{class:l[4]}];function a(u){l[17](u)}let i={$$slots:{default:[Dt]},$$scope:{ctx:l}};for(let u=0;u<s.length;u+=1)i=B(i,s[u]);return l[0]!==void 0&&(i.open=l[0]),e=new ft({props:i}),Te.push(()=>Re(e,"open",a)),e.$on("show",l[18]),{c(){K(e.$$.fragment)},l(u){R(e.$$.fragment,u)},m(u,o){O(e,u,o),n=!0},p(u,[o]){const r=o&48?ce(s,[s[0],o&32&&tt(u[5]),o&16&&{class:u[4]}]):{};o&524366&&(r.$$scope={dirty:o,ctx:u}),!t&&o&1&&(t=!0,r.open=u[0],Ke(()=>t=!1)),e.$set(r)},i(u){n||(P(e.$$.fragment,u),n=!0)},o(u){V(e.$$.fragment,u),n=!1},d(u){Q(e,u)}}}function qt(l,e,t){let n,s,a,i;const u=["activeUrl","open","containerClass","classContainer","headerClass","classHeader","footerClass","classFooter","activeClass","classActive"];let o=$(e,u),{$$slots:r={},$$scope:c}=e;const d=Xe(r),h=mt("");let{activeUrl:k=""}=e,{open:v=!1}=e,{containerClass:b="divide-y z-50"}=e,{classContainer:y=void 0}=e,{headerClass:T="py-1 overflow-hidden rounded-t-lg"}=e,{classHeader:q=void 0}=e,{footerClass:z="py-1 overflow-hidden rounded-b-lg"}=e,{classFooter:H=void 0}=e,{activeClass:N="text-primary-700 dark:text-primary-700 hover:text-primary-900 dark:hover:text-primary-900"}=e,{classActive:g=void 0}=e,L=W(N,g);ye("DropdownType",{activeClass:L}),ye("activeUrl",h);function C(f){v=f,t(0,v)}function _(f){S.call(this,l,f)}return l.$$set=f=>{t(22,e=B(B({},e),ee(f))),t(5,o=$(e,u)),"activeUrl"in f&&t(7,k=f.activeUrl),"open"in f&&t(0,v=f.open),"containerClass"in f&&t(8,b=f.containerClass),"classContainer"in f&&t(9,y=f.classContainer),"headerClass"in f&&t(10,T=f.headerClass),"classHeader"in f&&t(11,q=f.classHeader),"footerClass"in f&&t(12,z=f.footerClass),"classFooter"in f&&t(13,H=f.classFooter),"activeClass"in f&&t(14,N=f.activeClass),"classActive"in f&&t(15,g=f.classActive),"$$scope"in f&&t(19,c=f.$$scope)},l.$$.update=()=>{l.$$.dirty&128&&h.set(k),l.$$.dirty&768&&t(4,n=W(b,y)),l.$$.dirty&3072&&t(3,s=W(T,q)),t(2,a=W("py-1",e.class)),l.$$.dirty&12288&&t(1,i=W(z,H)),t(5,o.arrow=o.arrow??!1,o),t(5,o.trigger=o.trigger??"click",o),t(5,o.placement=o.placement??"bottom",o),t(5,o.color=o.color??"dropdown",o),t(5,o.shadow=o.shadow??!0,o),t(5,o.rounded=o.rounded??!0,o)},e=ee(e),[v,i,a,s,n,o,d,k,b,y,T,q,z,H,N,g,r,C,_,c]}class Lt extends J{constructor(e){super(),x(this,e,qt,It,G,{activeUrl:7,open:0,containerClass:8,classContainer:9,headerClass:10,classHeader:11,footerClass:12,classFooter:13,activeClass:14,classActive:15})}}function _e(l){let e,t,n,s,a,i;const u=l[9].default,o=le(u,l,l[18],null);let r=[{href:l[0]},{type:t=l[0]?void 0:"button"},{role:n=l[0]?"link":"button"},l[4],{class:l[2]}],c={};for(let d=0;d<r.length;d+=1)c=B(c,r[d]);return{c(){e=D(l[0]?"a":"button"),o&&o.c(),this.h()},l(d){e=I(d,((l[0]?"a":"button")||"null").toUpperCase(),{href:!0,type:!0,role:!0,class:!0});var h=E(e);o&&o.l(h),h.forEach(p),this.h()},h(){pe(l[0]?"a":"button")(e,c)},m(d,h){M(d,e,h),o&&o.m(e,null),s=!0,a||(i=[A(e,"click",l[10]),A(e,"change",l[11]),A(e,"keydown",l[12]),A(e,"keyup",l[13]),A(e,"focus",l[14]),A(e,"blur",l[15]),A(e,"mouseenter",l[16]),A(e,"mouseleave",l[17])],a=!0)},p(d,h){o&&o.p&&(!s||h&262144)&&ae(o,u,d,d[18],s?ie(u,d[18],h,null):ne(d[18]),null),pe(d[0]?"a":"button")(e,c=ce(r,[(!s||h&1)&&{href:d[0]},(!s||h&1&&t!==(t=d[0]?void 0:"button"))&&{type:t},(!s||h&1&&n!==(n=d[0]?"link":"button"))&&{role:n},h&16&&d[4],(!s||h&4)&&{class:d[2]}]))},i(d){s||(P(o,d),s=!0)},o(d){V(o,d),s=!1},d(d){d&&p(e),o&&o.d(d),a=!1,ve(i)}}}function Pt(l){let e=l[0]?"a":"button",t,n,s=(l[0]?"a":"button")&&_e(l);return{c(){s&&s.c(),t=j()},l(a){s&&s.l(a),t=j()},m(a,i){s&&s.m(a,i),M(a,t,i),n=!0},p(a,i){a[0],e?G(e,a[0]?"a":"button")?(s.d(1),s=_e(a),e=a[0]?"a":"button",s.c(),s.m(t.parentNode,t)):s.p(a,i):(s=_e(a),e=a[0]?"a":"button",s.c(),s.m(t.parentNode,t))},i(a){n||(P(s,a),n=!0)},o(a){V(s,a),n=!1},d(a){a&&p(t),s&&s.d(a)}}}function Mt(l){let e,t;return e=new _t({props:{tag:"li",show:l[1],use:l[3],$$slots:{default:[Pt]},$$scope:{ctx:l}}}),{c(){K(e.$$.fragment)},l(n){R(e.$$.fragment,n)},m(n,s){O(e,n,s),t=!0},p(n,[s]){const a={};s&2&&(a.show=n[1]),s&262165&&(a.$$scope={dirty:s,ctx:n}),e.$set(a)},i(n){t||(P(e.$$.fragment,n),t=!0)},o(n){V(e.$$.fragment,n),t=!1},d(n){Q(e,n)}}}function Vt(l,e,t){let n,s;const a=["defaultClass","href","activeClass"];let i=$(e,a),{$$slots:u={},$$scope:o}=e,{defaultClass:r="font-medium py-2 px-4 text-sm hover:bg-gray-100 dark:hover:bg-gray-600"}=e,{href:c=void 0}=e,{activeClass:d=void 0}=e;const h=we("DropdownType")??{},k=we("activeUrl");let v="";k.subscribe(_=>{t(7,v=_)});let b=!0;function y(_){var f;t(1,b=((f=_.parentElement)==null?void 0:f.tagName)==="UL")}function T(_){S.call(this,l,_)}function q(_){S.call(this,l,_)}function z(_){S.call(this,l,_)}function H(_){S.call(this,l,_)}function N(_){S.call(this,l,_)}function g(_){S.call(this,l,_)}function L(_){S.call(this,l,_)}function C(_){S.call(this,l,_)}return l.$$set=_=>{t(21,e=B(B({},e),ee(_))),t(4,i=$(e,a)),"defaultClass"in _&&t(5,r=_.defaultClass),"href"in _&&t(0,c=_.href),"activeClass"in _&&t(6,d=_.activeClass),"$$scope"in _&&t(18,o=_.$$scope)},l.$$.update=()=>{l.$$.dirty&129&&t(8,n=v?c===v:!1),t(2,s=W(r,c?"block":"w-full text-left",n&&(d??h.activeClass),e.class))},e=ee(e),[c,b,s,y,i,r,d,v,n,u,T,q,z,H,N,g,L,C,o]}class zt extends J{constructor(e){super(),x(this,e,Vt,Mt,G,{defaultClass:5,href:0,activeClass:6})}}function At(l){let e,t=`Changes the output 
probability distribution 
and randomness of next token.`,n;return{c(){e=D("div"),n=X(t),this.h()},l(s){e=I(s,"DIV",{class:!0});var a=E(e);n=Z(a,t),a.forEach(p),this.h()},h(){m(e,"class","h-5")},m(s,a){M(s,e,a),w(e,n)},p:Y,d(s){s&&p(e)}}}function Ht(l){let e,t,n,s,a,i="Temperature",u,o,r,c,d,h,k,v,b,y,T;return o=new pt({props:{id:"temperature-help",placement:"right",$$slots:{default:[At]},$$scope:{ctx:l}}}),{c(){e=D("div"),t=D("div"),n=D("div"),s=D("div"),a=D("div"),a.textContent=i,u=U(),K(o.$$.fragment),r=U(),c=D("div"),d=D("p"),h=X(l[2]),k=U(),v=D("input"),this.h()},l(q){e=I(q,"DIV",{class:!0});var z=E(e);t=I(z,"DIV",{class:!0});var H=E(t);n=I(H,"DIV",{class:!0});var N=E(n);s=I(N,"DIV",{class:!0});var g=E(s);a=I(g,"DIV",{"data-svelte-h":!0}),re(a)!=="svelte-knfc80"&&(a.textContent=i),u=F(g),R(o.$$.fragment,g),g.forEach(p),r=F(N),c=I(N,"DIV",{class:!0});var L=E(c);d=I(L,"P",{});var C=E(d);h=Z(C,l[2]),C.forEach(p),L.forEach(p),N.forEach(p),k=F(H),v=I(H,"INPUT",{class:!0,type:!0,min:!0,max:!0,step:!0}),H.forEach(p),z.forEach(p),this.h()},h(){m(s,"class","temperature-text flex items-center gap-[2px] svelte-1cjlkip"),m(c,"class","temperature-value svelte-1cjlkip"),m(n,"class","flex w-full shrink-0 items-center justify-between"),v.disabled=l[0],m(v,"class","slider svelte-1cjlkip"),m(v,"type","range"),m(v,"min",0),m(v,"max",l[3].length-1),m(v,"step",1),m(t,"class","slider-container flex w-full flex-col items-end svelte-1cjlkip"),m(e,"class","temperature-slider text-gray-900 svelte-1cjlkip")},m(q,z){M(q,e,z),w(e,t),w(t,n),w(n,s),w(s,a),w(s,u),O(o,s,null),w(n,r),w(n,c),w(c,d),w(d,h),w(t,k),w(t,v),Ie(v,l[1]),b=!0,y||(T=[A(v,"change",l[4]),A(v,"input",l[4]),A(v,"click",Nt)],y=!0)},p(q,[z]){const H={};z&128&&(H.$$scope={dirty:z,ctx:q}),o.$set(H),(!b||z&4)&&oe(h,q[2]),(!b||z&1)&&(v.disabled=q[0]),z&2&&Ie(v,q[1])},i(q){b||(P(o.$$.fragment,q),b=!0)},o(q){V(o.$$.fragment,q),b=!1},d(q){q&&p(e),Q(o),y=!1,ve(T)}}}const Nt=l=>{l.preventDefault(),l.stopPropagation()};function St(l,e,t){let n,{disabled:s=!1}=e,a=8;const i=[.2,.3,.4,.5,.6,.7,.8,.9,1,2,3,4,5,6,7,8,9,10];i.length-1;function u(){a=Ze(this.value),t(1,a)}return l.$$set=o=>{"disabled"in o&&t(0,s=o.disabled)},l.$$.update=()=>{l.$$.dirty&2&&t(2,n=i[a]),l.$$.dirty&4&&st.set(n)},[s,a,n,i,u]}class Ut extends J{constructor(e){super(),x(this,e,St,Ht,G,{disabled:0})}}function Ft(l){let e,t,n,s,a,i=l[4].id&&l[4].title&&Ne(l),u=l[6].id&&l[6].desc&&Se(l),o=[{xmlns:"http://www.w3.org/2000/svg"},{fill:"none"},{color:l[2]},l[11],{class:s=W("shrink-0",l[9][l[0]??"md"],l[12].class)},{role:l[1]},{"aria-label":l[7]},{"aria-describedby":a=l[8]?l[10]:void 0},{viewBox:"0 0 24 24"}],r={};for(let c=0;c<o.length;c+=1)r=B(r,o[c]);return{c(){e=te("svg"),i&&i.c(),t=j(),u&&u.c(),n=te("path"),this.h()},l(c){e=se(c,"svg",{xmlns:!0,fill:!0,color:!0,class:!0,role:!0,"aria-label":!0,"aria-describedby":!0,viewBox:!0});var d=E(e);i&&i.l(d),t=j(),u&&u.l(d),n=se(d,"path",{stroke:!0,"stroke-linecap":!0,"stroke-linejoin":!0,"stroke-width":!0,d:!0}),E(n).forEach(p),d.forEach(p),this.h()},h(){m(n,"stroke","currentColor"),m(n,"stroke-linecap","round"),m(n,"stroke-linejoin","round"),m(n,"stroke-width",l[5]),m(n,"d","m8 10 4 4 4-4"),me(e,r)},m(c,d){M(c,e,d),i&&i.m(e,null),w(e,t),u&&u.m(e,null),w(e,n)},p(c,d){c[4].id&&c[4].title?i?i.p(c,d):(i=Ne(c),i.c(),i.m(e,t)):i&&(i.d(1),i=null),c[6].id&&c[6].desc?u?u.p(c,d):(u=Se(c),u.c(),u.m(e,n)):u&&(u.d(1),u=null),d&32&&m(n,"stroke-width",c[5]),me(e,r=ce(o,[{xmlns:"http://www.w3.org/2000/svg"},{fill:"none"},d&4&&{color:c[2]},d&2048&&c[11],d&4097&&s!==(s=W("shrink-0",c[9][c[0]??"md"],c[12].class))&&{class:s},d&2&&{role:c[1]},d&128&&{"aria-label":c[7]},d&256&&a!==(a=c[8]?c[10]:void 0)&&{"aria-describedby":a},{viewBox:"0 0 24 24"}]))},d(c){c&&p(e),i&&i.d(),u&&u.d()}}}function jt(l){let e,t,n,s,a,i,u,o=l[4].id&&l[4].title&&Ue(l),r=l[6].id&&l[6].desc&&Fe(l),c=[{xmlns:"http://www.w3.org/2000/svg"},{fill:"none"},{color:l[2]},l[11],{class:s=W("shrink-0",l[9][l[0]??"md"],l[12].class)},{role:l[1]},{"aria-label":l[7]},{"aria-describedby":a=l[8]?l[10]:void 0},{viewBox:"0 0 24 24"}],d={};for(let h=0;h<c.length;h+=1)d=B(d,c[h]);return{c(){e=te("svg"),o&&o.c(),t=j(),r&&r.c(),n=te("path"),this.h()},l(h){e=se(h,"svg",{xmlns:!0,fill:!0,color:!0,class:!0,role:!0,"aria-label":!0,"aria-describedby":!0,viewBox:!0});var k=E(e);o&&o.l(k),t=j(),r&&r.l(k),n=se(k,"path",{stroke:!0,"stroke-linecap":!0,"stroke-linejoin":!0,"stroke-width":!0,d:!0}),E(n).forEach(p),k.forEach(p),this.h()},h(){m(n,"stroke","currentColor"),m(n,"stroke-linecap","round"),m(n,"stroke-linejoin","round"),m(n,"stroke-width",l[5]),m(n,"d","m8 10 4 4 4-4"),me(e,d)},m(h,k){M(h,e,k),o&&o.m(e,null),w(e,t),r&&r.m(e,null),w(e,n),i||(u=[A(e,"click",l[13]),A(e,"keydown",l[14]),A(e,"keyup",l[15]),A(e,"focus",l[16]),A(e,"blur",l[17]),A(e,"mouseenter",l[18]),A(e,"mouseleave",l[19]),A(e,"mouseover",l[20]),A(e,"mouseout",l[21])],i=!0)},p(h,k){h[4].id&&h[4].title?o?o.p(h,k):(o=Ue(h),o.c(),o.m(e,t)):o&&(o.d(1),o=null),h[6].id&&h[6].desc?r?r.p(h,k):(r=Fe(h),r.c(),r.m(e,n)):r&&(r.d(1),r=null),k&32&&m(n,"stroke-width",h[5]),me(e,d=ce(c,[{xmlns:"http://www.w3.org/2000/svg"},{fill:"none"},k&4&&{color:h[2]},k&2048&&h[11],k&4097&&s!==(s=W("shrink-0",h[9][h[0]??"md"],h[12].class))&&{class:s},k&2&&{role:h[1]},k&128&&{"aria-label":h[7]},k&256&&a!==(a=h[8]?h[10]:void 0)&&{"aria-describedby":a},{viewBox:"0 0 24 24"}]))},d(h){h&&p(e),o&&o.d(),r&&r.d(),i=!1,ve(u)}}}function Ne(l){let e,t=l[4].title+"",n,s;return{c(){e=te("title"),n=X(t),this.h()},l(a){e=se(a,"title",{id:!0});var i=E(e);n=Z(i,t),i.forEach(p),this.h()},h(){m(e,"id",s=l[4].id)},m(a,i){M(a,e,i),w(e,n)},p(a,i){i&16&&t!==(t=a[4].title+"")&&oe(n,t),i&16&&s!==(s=a[4].id)&&m(e,"id",s)},d(a){a&&p(e)}}}function Se(l){let e,t=l[6].desc+"",n,s;return{c(){e=te("desc"),n=X(t),this.h()},l(a){e=se(a,"desc",{id:!0});var i=E(e);n=Z(i,t),i.forEach(p),this.h()},h(){m(e,"id",s=l[6].id)},m(a,i){M(a,e,i),w(e,n)},p(a,i){i&64&&t!==(t=a[6].desc+"")&&oe(n,t),i&64&&s!==(s=a[6].id)&&m(e,"id",s)},d(a){a&&p(e)}}}function Ue(l){let e,t=l[4].title+"",n,s;return{c(){e=te("title"),n=X(t),this.h()},l(a){e=se(a,"title",{id:!0});var i=E(e);n=Z(i,t),i.forEach(p),this.h()},h(){m(e,"id",s=l[4].id)},m(a,i){M(a,e,i),w(e,n)},p(a,i){i&16&&t!==(t=a[4].title+"")&&oe(n,t),i&16&&s!==(s=a[4].id)&&m(e,"id",s)},d(a){a&&p(e)}}}function Fe(l){let e,t=l[6].desc+"",n,s;return{c(){e=te("desc"),n=X(t),this.h()},l(a){e=se(a,"desc",{id:!0});var i=E(e);n=Z(i,t),i.forEach(p),this.h()},h(){m(e,"id",s=l[6].id)},m(a,i){M(a,e,i),w(e,n)},p(a,i){i&64&&t!==(t=a[6].desc+"")&&oe(n,t),i&64&&s!==(s=a[6].id)&&m(e,"id",s)},d(a){a&&p(e)}}}function Bt(l){let e;function t(a,i){return a[3]?jt:Ft}let n=t(l),s=n(l);return{c(){s.c(),e=j()},l(a){s.l(a),e=j()},m(a,i){s.m(a,i),M(a,e,i)},p(a,[i]){n===(n=t(a))&&s?s.p(a,i):(s.d(1),s=n(a),s&&(s.c(),s.m(e.parentNode,e)))},i:Y,o:Y,d(a){a&&p(e),s.d(a)}}}function Gt(l,e,t){const n=["size","role","color","withEvents","title","strokeWidth","desc","ariaLabel"];let s=$(e,n);const a=we("iconCtx")??{},i={xs:"w-3 h-3",sm:"w-4 h-4",md:"w-5 h-5",lg:"w-6 h-6",xl:"w-8 h-8"};let{size:u=a.size||"md"}=e,{role:o=a.role||"img"}=e,{color:r=a.color||"currentColor"}=e,{withEvents:c=a.withEvents||!1}=e,{title:d={}}=e,{strokeWidth:h=a.strokeWidth||"2"}=e,{desc:k={}}=e,v=`${d.id||""} ${k.id||""}`,b=!1,{ariaLabel:y="chevron down outline"}=e;function T(f){S.call(this,l,f)}function q(f){S.call(this,l,f)}function z(f){S.call(this,l,f)}function H(f){S.call(this,l,f)}function N(f){S.call(this,l,f)}function g(f){S.call(this,l,f)}function L(f){S.call(this,l,f)}function C(f){S.call(this,l,f)}function _(f){S.call(this,l,f)}return l.$$set=f=>{t(12,e=B(B({},e),ee(f))),t(11,s=$(e,n)),"size"in f&&t(0,u=f.size),"role"in f&&t(1,o=f.role),"color"in f&&t(2,r=f.color),"withEvents"in f&&t(3,c=f.withEvents),"title"in f&&t(4,d=f.title),"strokeWidth"in f&&t(5,h=f.strokeWidth),"desc"in f&&t(6,k=f.desc),"ariaLabel"in f&&t(7,y=f.ariaLabel)},l.$$.update=()=>{l.$$.dirty&80&&(d.id||k.id?t(8,b=!0):t(8,b=!1))},e=ee(e),[u,o,r,c,d,h,k,y,b,i,v,s,e,T,q,z,H,N,g,L,C,_]}class Wt extends J{constructor(e){super(),x(this,e,Gt,Bt,G,{size:0,role:1,color:2,withEvents:3,title:4,strokeWidth:5,desc:6,ariaLabel:7})}}function Kt(l){let e,t='<div class="dot svelte-14pkmx6"></div> <div class="dot svelte-14pkmx6"></div> <div class="dot svelte-14pkmx6"></div>';return{c(){e=D("div"),e.innerHTML=t,this.h()},l(n){e=I(n,"DIV",{class:!0,"data-svelte-h":!0}),re(e)!=="svelte-1lxxy8o"&&(e.innerHTML=t),this.h()},h(){m(e,"class","container svelte-14pkmx6")},m(n,s){M(n,e,s)},p:Y,i:Y,o:Y,d(n){n&&p(e)}}}function Rt(l){return Ye(()=>{const e=lt.timeline({repeat:-1});e.fromTo(".dot",{y:3},{y:-3,duration:.3,stagger:.1}),e.to(".dot",{y:3,duration:.3,stagger:.1})}),[]}class Ot extends J{constructor(e){super(),x(this,e,Rt,Kt,G,{})}}function je(l,e,t){const n=l.slice();return n[19]=e[t],n[21]=t,n}function Qt(l){let e;return{c(){e=X(l[19])},l(t){e=Z(t,l[19])},m(t,n){M(t,e,n)},p:Y,d(t){t&&p(e)}}}function Be(l){let e,t;function n(){return l[15](l[19],l[21])}return e=new zt({props:{$$slots:{default:[Qt]},$$scope:{ctx:l}}}),e.$on("click",n),{c(){K(e.$$.fragment)},l(s){R(e.$$.fragment,s)},m(s,a){O(e,s,a),t=!0},p(s,a){l=s;const i={};a&4194304&&(i.$$scope={dirty:a,ctx:l}),e.$set(i)},i(s){t||(P(e.$$.fragment,s),t=!0)},o(s){V(e.$$.fragment,s),t=!1},d(s){Q(e,s)}}}function Xt(l){let e,t,n=Pe(Me),s=[];for(let i=0;i<n.length;i+=1)s[i]=Be(je(l,n,i));const a=i=>V(s[i],1,1,()=>{s[i]=null});return{c(){for(let i=0;i<s.length;i+=1)s[i].c();e=j()},l(i){for(let u=0;u<s.length;u+=1)s[u].l(i);e=j()},m(i,u){for(let o=0;o<s.length;o+=1)s[o]&&s[o].m(i,u);M(i,e,u),t=!0},p(i,u){if(u&2048){n=Pe(Me);let o;for(o=0;o<n.length;o+=1){const r=je(i,n,o);s[o]?(s[o].p(r,u),P(s[o],1)):(s[o]=Be(r),s[o].c(),P(s[o],1),s[o].m(e.parentNode,e))}for(de(),o=n.length;o<s.length;o+=1)a(o);he()}},i(i){if(!t){for(let u=0;u<n.length;u+=1)P(s[u]);t=!0}},o(i){s=s.filter(Boolean);for(let u=0;u<s.length;u+=1)V(s[u]);t=!1},d(i){i&&p(e),Je(s,i)}}}function Ge(l){let e,t,n;return t=new Ot({}),{c(){e=D("div"),K(t.$$.fragment),this.h()},l(s){e=I(s,"DIV",{class:!0});var a=E(e);R(t.$$.fragment,a),a.forEach(p),this.h()},h(){m(e,"class","loading svelte-gwqyih")},m(s,a){M(s,e,a),O(t,e,null),n=!0},i(s){n||(P(t.$$.fragment,s),n=!0)},o(s){V(t.$$.fragment,s),n=!1},d(s){s&&p(e),Q(t)}}}function We(l){let e,t="Try the examples while GPT-2 model is being downloaded (600MB)";return{c(){e=D("span"),e.textContent=t,this.h()},l(n){e=I(n,"SPAN",{class:!0,"data-svelte-h":!0}),re(e)!=="svelte-d2s81t"&&(e.textContent=t),this.h()},h(){m(e,"class","helper-text svelte-gwqyih")},m(n,s){M(n,e,s)},d(n){n&&p(e)}}}function Zt(l){let e,t,n,s,a,i,u,o,r,c,d,h,k,v,b,y,T,q,z;n=new Wt({props:{class:"pointer-events-none h-4 w-4 text-gray-500"}});function H(C){l[16](C)}let N={placement:"bottom-start",$$slots:{default:[Xt]},$$scope:{ctx:l}};l[3]!==void 0&&(N.open=l[3]),a=new Lt({props:N}),Te.push(()=>Re(a,"open",H));let g=l[5]&&Ge(),L=l[0]&&We();return{c(){e=D("button"),t=X("Examples"),K(n.$$.fragment),s=U(),K(a.$$.fragment),u=U(),o=D("div"),r=D("div"),c=D("span"),d=X(l[2]),h=D("span"),k=X(l[6]),b=U(),g&&g.c(),y=U(),L&&L.c(),this.h()},l(C){e=I(C,"BUTTON",{type:!0,class:!0});var _=E(e);t=Z(_,"Examples"),R(n.$$.fragment,_),_.forEach(p),s=F(C),R(a.$$.fragment,C),u=F(C),o=I(C,"DIV",{class:!0});var f=E(o);r=I(f,"DIV",{contenteditable:!0,class:!0,placeholder:!0,role:!0});var ge=E(r);c=I(ge,"SPAN",{class:!0});var Ce=E(c);d=Z(Ce,l[2]),Ce.forEach(p),h=I(ge,"SPAN",{class:!0});var Ee=E(h);k=Z(Ee,l[6]),Ee.forEach(p),ge.forEach(p),b=F(f),g&&g.l(f),y=F(f),L&&L.l(f),f.forEach(p),this.h()},h(){m(e,"type","button"),e.disabled=l[4],m(e,"class","select-button inline-flex items-center justify-center border border-s-0 border-gray-200 bg-white px-3 py-2 text-center text-xs font-medium text-gray-900 first:rounded-s-lg first:border-s last:rounded-e-lg svelte-gwqyih"),fe(e,"disabled",l[4]),m(c,"class","user-input"),m(h,"class","predicted svelte-gwqyih"),m(r,"contenteditable",v=!l[4]),m(r,"class","input-box svelte-gwqyih"),m(r,"placeholder","Test your own input text"),m(r,"role","input"),m(o,"class","input-container relative svelte-gwqyih"),fe(o,"disabled",l[4])},m(C,_){M(C,e,_),w(e,t),O(n,e,null),M(C,s,_),O(a,C,_),M(C,u,_),M(C,o,_),w(o,r),w(r,c),w(c,d),w(r,h),w(h,k),l[17](r),w(o,b),g&&g.m(o,null),w(o,y),L&&L.m(o,null),T=!0,q||(z=[A(r,"focus",l[7]),A(r,"input",l[8]),A(r,"keydown",l[10])],q=!0)},p(C,_){(!T||_&16)&&(e.disabled=C[4]),(!T||_&16)&&fe(e,"disabled",C[4]);const f={};_&4194304&&(f.$$scope={dirty:_,ctx:C}),!i&&_&8&&(i=!0,f.open=C[3],Ke(()=>i=!1)),a.$set(f),(!T||_&4)&&oe(d,C[2]),(!T||_&64)&&oe(k,C[6]),(!T||_&16&&v!==(v=!C[4]))&&m(r,"contenteditable",v),C[5]?g?_&32&&P(g,1):(g=Ge(),g.c(),P(g,1),g.m(o,y)):g&&(de(),V(g,1,1,()=>{g=null}),he()),C[0]?L||(L=We(),L.c(),L.m(o,null)):L&&(L.d(1),L=null),(!T||_&16)&&fe(o,"disabled",C[4])},i(C){T||(P(n.$$.fragment,C),P(a.$$.fragment,C),P(g),T=!0)},o(C){V(n.$$.fragment,C),V(a.$$.fragment,C),V(g),T=!1},d(C){C&&(p(e),p(s),p(u),p(o)),Q(n),Q(a,C),l[17](null),g&&g.d(),L&&L.d(),q=!1,ve(z)}}}function Yt(l){let e,t,n,s,a,i,u,o,r,c,d,h,k;return s=new Tt({props:{class:"w-full grow",size:"sm",$$slots:{default:[Zt]},$$scope:{ctx:l}}}),c=new Ut({props:{disabled:l[5]}}),{c(){e=D("div"),t=D("div"),n=D("form"),K(s.$$.fragment),a=U(),i=D("button"),u=X("Generate"),r=U(),K(c.$$.fragment),this.h()},l(v){e=I(v,"DIV",{class:!0});var b=E(e);t=I(b,"DIV",{class:!0});var y=E(t);n=I(y,"FORM",{class:!0});var T=E(n);R(s.$$.fragment,T),a=F(T),i=I(T,"BUTTON",{class:!0,type:!0});var q=E(i);u=Z(q,"Generate"),q.forEach(p),T.forEach(p),y.forEach(p),r=F(b),R(c.$$.fragment,b),b.forEach(p),this.h()},h(){i.disabled=l[4],m(i,"class",o=qe(Le("generate-button rounded-lg text-center text-sm shadow-sm",{disabled:l[4],active:!l[4]}))+" svelte-gwqyih"),m(i,"type","submit"),m(n,"class","flex w-full items-center gap-2"),m(t,"class","flex flex-1 items-center gap-1 whitespace-nowrap"),m(e,"class","input-area flex flex-shrink-0 svelte-gwqyih")},m(v,b){M(v,e,b),w(e,t),w(t,n),O(s,n,null),w(n,a),w(n,i),w(i,u),w(e,r),O(c,e,null),d=!0,h||(k=A(i,"click",l[9]),h=!0)},p(v,[b]){const y={};b&4194431&&(y.$$scope={dirty:b,ctx:v}),s.$set(y),(!d||b&16)&&(i.disabled=v[4]),(!d||b&16&&o!==(o=qe(Le("generate-button rounded-lg text-center text-sm shadow-sm",{disabled:v[4],active:!v[4]}))+" svelte-gwqyih"))&&m(i,"class",o);const T={};b&32&&(T.disabled=v[5]),c.$set(T)},i(v){d||(P(s.$$.fragment,v),P(c.$$.fragment,v),d=!0)},o(v){V(s.$$.fragment,v),V(c.$$.fragment,v),d=!1},d(v){v&&p(e),Q(s),Q(c),h=!1,k()}}}function Jt(l,e,t){let n,s,a,i,u,o,r,c;ue(l,at,g=>t(12,i=g)),ue(l,nt,g=>t(13,u=g)),ue(l,it,g=>t(0,o=g)),ue(l,ot,g=>t(14,r=g)),ue(l,ke,g=>t(18,c=g));let d,h=c;const k=g=>{var L;t(6,n=""),t(2,h=(L=g.target)==null?void 0:L.textContent)},v=g=>{var C;const L=(C=g.target)==null?void 0:C.querySelector(".user-input");t(2,h=L.innerText)},b=g=>{var L;t(6,n=""),t(2,h=((L=d==null?void 0:d.textContent)==null?void 0:L.trim())||""),ke.set(h)},y=g=>{g.key==="Enter"&&b()};let T=!1;const q=(g,L)=>{t(3,T=!1),t(2,h=g),t(6,n=""),ke.set(g.trim()),rt.set(L)},z=(g,L)=>{q(g,L)};function H(g){T=g,t(3,T)}function N(g){Te[g?"unshift":"push"](()=>{d=g,t(1,d)})}return l.$$.update=()=>{l.$$.dirty&16384&&t(6,n=(r==null?void 0:r.token)||""),l.$$.dirty&8193&&t(5,s=o||u),l.$$.dirty&12289&&t(4,a=o||u||i.id!==null)},[o,d,h,T,a,s,n,k,v,b,y,q,i,u,r,z,H,N]}class xt extends J{constructor(e){super(),x(this,e,Jt,Yt,G,{})}}function $t(l){let e,t,n='T<span class="small svelte-15vq1w1">RANSFORMER</span> E<span class="small svelte-15vq1w1">XPLAINER</span>',s,a,i,u,o,r,c,d='<svg class="h-6 w-6 text-gray-800 dark:text-white svelte-15vq1w1" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M5 3a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h11.5c.07 0 .14-.007.207-.021.095.014.193.021.293.021h2a2 2 0 0 0 2-2V7a1 1 0 0 0-1-1h-1a1 1 0 1 0 0 2v11h-2V5a2 2 0 0 0-2-2H5Zm7 4a1 1 0 0 1 1-1h.5a1 1 0 1 1 0 2H13a1 1 0 0 1-1-1Zm0 3a1 1 0 0 1 1-1h.5a1 1 0 1 1 0 2H13a1 1 0 0 1-1-1Zm-6 4a1 1 0 0 1 1-1h6a1 1 0 1 1 0 2H7a1 1 0 0 1-1-1Zm0 3a1 1 0 0 1 1-1h6a1 1 0 1 1 0 2H7a1 1 0 0 1-1-1ZM7 6a1 1 0 0 0-1 1v3a1 1 0 0 0 1 1h3a1 1 0 0 0 1-1V7a1 1 0 0 0-1-1H7Zm1 3V8h1v1H8Z" clip-rule="evenodd"></path></svg> <a href="https://github.com/poloclub/transformer-explainer" target="_blank"><svg class="h-6 w-6 text-gray-800 dark:text-white svelte-15vq1w1" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M12.006 2a9.847 9.847 0 0 0-6.484 2.44 10.32 10.32 0 0 0-3.393 6.17 10.48 10.48 0 0 0 1.317 6.955 10.045 10.045 0 0 0 5.4 4.418c.504.095.683-.223.683-.494 0-.245-.01-1.052-.014-1.908-2.78.62-3.366-1.21-3.366-1.21a2.711 2.711 0 0 0-1.11-1.5c-.907-.637.07-.621.07-.621.317.044.62.163.885.346.266.183.487.426.647.71.135.253.318.476.538.655a2.079 2.079 0 0 0 2.37.196c.045-.52.27-1.006.635-1.37-2.219-.259-4.554-1.138-4.554-5.07a4.022 4.022 0 0 1 1.031-2.75 3.77 3.77 0 0 1 .096-2.713s.839-.275 2.749 1.05a9.26 9.26 0 0 1 5.004 0c1.906-1.325 2.74-1.05 2.74-1.05.37.858.406 1.828.101 2.713a4.017 4.017 0 0 1 1.029 2.75c0 3.939-2.339 4.805-4.564 5.058a2.471 2.471 0 0 1 .679 1.897c0 1.372-.012 2.477-.012 2.814 0 .272.18.592.687.492a10.05 10.05 0 0 0 5.388-4.421 10.473 10.473 0 0 0 1.313-6.948 10.32 10.32 0 0 0-3.39-6.165A9.847 9.847 0 0 0 12.007 2Z" clip-rule="evenodd"></path></svg></a>',h;return o=new xt({}),{c(){e=D("div"),t=D("div"),t.innerHTML=n,s=U(),a=D("div"),i=D("div"),u=D("div"),K(o.$$.fragment),r=U(),c=D("div"),c.innerHTML=d,this.h()},l(k){e=I(k,"DIV",{class:!0});var v=E(e);t=I(v,"DIV",{class:!0,"data-svelte-h":!0}),re(t)!=="svelte-793i68"&&(t.innerHTML=n),s=F(v),a=I(v,"DIV",{class:!0});var b=E(a);i=I(b,"DIV",{class:!0});var y=E(i);u=I(y,"DIV",{class:!0});var T=E(u);R(o.$$.fragment,T),T.forEach(p),y.forEach(p),b.forEach(p),r=F(v),c=I(v,"DIV",{class:!0,"data-svelte-h":!0}),re(c)!=="svelte-1ktv31k"&&(c.innerHTML=d),v.forEach(p),this.h()},h(){m(t,"class","logo text-bold text-gray-700 svelte-15vq1w1"),m(u,"class","grow"),m(i,"class","flex w-full"),m(a,"class","inputs flex grow items-center"),m(c,"class","icons flex items-center gap-4"),m(e,"class","top-bar flex w-full items-center gap-4 px-10 py-2 svelte-15vq1w1")},m(k,v){M(k,e,v),w(e,t),w(e,s),w(e,a),w(a,i),w(i,u),O(o,u,null),w(e,r),w(e,c),h=!0},p:Y,i(k){h||(P(o.$$.fragment,k),h=!0)},o(k){V(o.$$.fragment,k),h=!1},d(k){k&&p(e),Q(o)}}}class es extends J{constructor(e){super(),x(this,e,null,$t,G,{})}}function ts(l){let e,t=`<div class="article-section svelte-126cykp"><h1 class="svelte-126cykp">What is a Transformer?</h1> <p class="svelte-126cykp">Transformer is a neural network architecture that has fundamentally changed the approach to
			Artificial Intelligence. Transformer was first introduced in the seminal paper
			<a href="https://dl.acm.org/doi/10.5555/3295222.3295349" title="ACM Digital Library" target="_blank" class="svelte-126cykp">&quot;Attention is All You Need&quot;</a>
			in 2017 and has since become the go-to architecture for deep learning models, powering text-generative
			models like OpenAI&#39;s <strong>GPT</strong>, Meta&#39;s <strong>Llama</strong>, and Google&#39;s
			<strong>Gemini</strong>. Beyond text, Transformer is also applied in
			<a href="https://huggingface.co/learn/audio-course/en/chapter3/introduction" title="Hugging Face" target="_blank" class="svelte-126cykp">audio generation</a>,
			<a href="https://huggingface.co/learn/computer-vision-course/unit3/vision-transformers/vision-transformers-for-image-classification" title="Hugging Face" target="_blank" class="svelte-126cykp">image recognition</a>,
			<a href="https://elifesciences.org/articles/82819" title="eLife" class="svelte-126cykp">protein structure prediction</a>, and even
			<a href="https://www.deeplearning.ai/the-batch/reinforcement-learning-plus-transformers-equals-efficiency/" title="Deep Learning AI" target="_blank" class="svelte-126cykp">game playing</a>, demonstrating their versatility across numerous domains.</p> <p class="svelte-126cykp">Fundamentally, text-generative Transformer models operate on the principle of <strong>next-word prediction</strong>: given a text prompt from the user, what is the <em>most probable next word</em> that will follow
			this input? The core innovation and power of Transformers lie in their use of self-attention mechanism,
			which allows them to process entire sequences and capture long-range dependencies more effectively
			than previous architectures.</p> <p class="svelte-126cykp">GPT-2 family of models are prominent examples of text-generative Transformers. Transformer
			Explainer is powered by the
			<a href="https://huggingface.co/openai-community/gpt2" title="Hugging Face" target="_blank" class="svelte-126cykp">GPT-2</a>
			(small) model which has 124 million parameter. While it is not the latest or most powerful Transformer
			model, it shares many of the same architectural components and principles found in the current
			state-of-the-art models making it an ideal starting point for understanding the basics.</p></div> <div class="article-section svelte-126cykp"><h1 class="svelte-126cykp">Transformer Architecture</h1> <p class="svelte-126cykp">Every text-generative Transformer consists of these three <strong>key components</strong>:</p> <ol class="svelte-126cykp"><li class="svelte-126cykp"><strong class="bold-purple svelte-126cykp">Embedding</strong>: Text input is divided into smaller units
				called tokens, which can be words or subwords. These tokens are converted into numerical
				vectors called embeddings, which capture the semantic meaning of words.</li> <li class="svelte-126cykp"><strong class="bold-purple svelte-126cykp">Transformer Block</strong> is the fundamental building block of
				the model that processes and transforms the input data. Each block includes:
				<ul class=" svelte-126cykp"><li class="svelte-126cykp"><strong>Attention Mechanism</strong>, the core component of the Transformer block. It
						allows tokens to communicate with other tokens, capturing contextual information and
						relationships between words.</li> <li class="svelte-126cykp"><strong>MLP (Multilayer Perceptron) Layer</strong>, a feed-forward network that operates
						on each token independently. While the goal of the attention layer is to route
						information between tokens, the goal of the MLP is to refine each token&#39;s
						representation.</li></ul></li> <li class="svelte-126cykp"><strong class="bold-purple svelte-126cykp">Output Probabilities</strong>: The final linear and softmax
				layers transform the processed embeddings into probabilities, enabling the model to make
				predictions about the next token in a sequence.</li></ol> <div class="architecture-section svelte-126cykp"><h2 class="svelte-126cykp">Embedding</h2> <p class="svelte-126cykp">Let&#39;s say you want to generate text using a Transformer model. You add the prompt like this
				one: <code class="svelte-126cykp">“Data visualization empowers users to”</code>. This input needs to be converted
				into a format that the model can understand and process. That is where embedding comes in:
				it transforms the text into a numerical representation that the model can work with. To
				convert a prompt into embedding, we need to 1) tokenize the input, 2) obtain token
				embeddings, 3) add positional information, and finally 4) add up token and position
				encodings to get the final embedding. Let’s see how each of these steps is done.</p> <div class="figure svelte-126cykp"><img src="./article_assets/embedding.png" width="60%" height="60%" align="middle" class="svelte-126cykp"/></div> <div class="figure-caption svelte-126cykp">Figure <span class="attention svelte-126cykp">X</span>. Expanding the Embedding layer view, showing how the
				input prompt is converted to a vector representation. The process involves
				<span class="fig-numbering">(1)</span> Tokenization, (2) Token Embedding, (3) Positional Encoding,
				and (4) Final Embedding.</div> <div class="article-subsection svelte-126cykp"><h3 class="svelte-126cykp">Step 1: Tokenization</h3> <p class="svelte-126cykp">Tokenization is the process of breaking down the input text into smaller, more manageable
					pieces called tokens. These tokens can be a word or a subword. The words <code class="svelte-126cykp">&quot;Data&quot;</code>
					and <code class="svelte-126cykp">&quot;vizualization&quot;</code> corresponds to unique tokens, while the word
					<code class="svelte-126cykp">&quot;empowers&quot;</code>
					is split into two tokens. The full vocabulary of tokens is decided before training the model:
					GPT-2&#39;s vocabulary has <code class="svelte-126cykp">50,257</code> unique tokens. Now that we split our input text
					into tokens with distinct IDs, we can obtain their vector representation from embeddings.</p></div> <div class="article-subsection svelte-126cykp"><h3 class="svelte-126cykp">Step 2. Token Embedding</h3> <p class="svelte-126cykp">GPT-2 Small represents each token in the vocabulary as a 768-dimensional vector; the
					dimension of the vector depends on the model. These embedding vectors are stored in a
					matrix of shape <code class="svelte-126cykp">(50,257, 768)</code>, containing approximately 39 million
					parameters! This extensive matrix allows the model to assign semantic meaning to each
					token.</p></div> <div class="article-subsection svelte-126cykp"><h3 class="svelte-126cykp">Step 3. Positional Encoding</h3> <p class="svelte-126cykp">The embedding layer also encodes information about each token&#39;s position in the input
					prompt. Different models use various methods for positional encoding. GPT-2 trains its own
					positional encoding matrix from scratch, integrating it directly into the training
					process.</p></div> <div class="article-subsection svelte-126cykp"><h3 class="svelte-126cykp">Step 4. Final Embedding</h3> <p class="svelte-126cykp">Finally, we sum the token and positional encodings to get the final embedding
					representation. This combined representation captures both the semantic meaning of the
					tokens and their position in the input sequence.</p></div></div> <div class="architecture-section svelte-126cykp"><h2 class="svelte-126cykp">Transformer Block</h2> <p class="svelte-126cykp">The core of the Transformer&#39;s processing lies in the Transformer block, which comprises
				multi-head self-attention and a Multi-Layer Perceptron layers. Most models consist of
				multiple such blocks that are stacked sequentially one after the other. The token
				representations evolve through layers, from the first block to the 12th one, allowing the
				model to build up an intricate understanding of each token. This layered approach leads to
				higher-order representations of the input.</p> <div class="article-subsection svelte-126cykp"><h3 class="svelte-126cykp">Multi-Head Self-Attention</h3> <p class="svelte-126cykp">The self-attention mechanism enables the model to focus on relevant parts of the input
					sequence, allowing it to capture complex relationships and dependencies within the data.
					Let’s look at how this self-attention is computed step-by-step.</p> <div class="article-subsection-l2 svelte-126cykp"><h4 class="svelte-126cykp">Step 1: Query, Key, and Value Matrices</h4> <div class="figure svelte-126cykp"><img src="./article_assets/QKV.png" width="80%" align="middle" class="svelte-126cykp"/></div> <div class="figure-caption svelte-126cykp">Figure <span class="attention svelte-126cykp">X</span>. Computing Query, Key, and Value matrices from
						the original embedding.</div> <p class="svelte-126cykp">Each token&#39;s embedding vector is transformed into three vectors:
						<span class="q-color svelte-126cykp">Query (Q)</span>,
						<span class="k-color svelte-126cykp">Key (K)</span>, and
						<span class="v-color svelte-126cykp">Value (V)</span>. These vectors are derived by multiplying the
						input embedding matrix with learned weight matrices for
						<span class="q-color svelte-126cykp">Q</span>,
						<span class="k-color svelte-126cykp">K</span>, and
						<span class="v-color svelte-126cykp">V</span>. Here&#39;s a web search analogy to help us build some
						intuition behind these matrices:</p> <ul class="svelte-126cykp"><li class="svelte-126cykp"><strong class="q-color font-medium svelte-126cykp">Query (Q)</strong> is the search text you type in
							the search engine bar. This is the token you want to
							<em>&quot;find more information about&quot;</em>.</li> <li class="svelte-126cykp"><strong class="k-color font-medium svelte-126cykp">Key (K)</strong> is the title of each web page in the
							search resul window. It represents the possible tokens the query can attend to.</li> <li class="svelte-126cykp"><strong class="v-color font-medium svelte-126cykp">Value (V)</strong> is the actual content of web pages
							shown. Once we matched the appropriate search term (Query) with the relevant results (Keys),
							we want to get the content (Value) of the most relevant pages.</li></ul> <p class="svelte-126cykp">By using these QKV values, the model can calculate attention scores, which determine how
						much focus each token should receive when generating predictions.</p></div> <div class="article-subsection-l2 svelte-126cykp"><h4 class="svelte-126cykp">Step 2: Masked Self-Attention</h4> <p class="svelte-126cykp">Masked self-attention allows the model to generate sequences by focusing on relevant
						parts of the input while preventing access to future tokens.</p> <div class="figure svelte-126cykp"><img src="./article_assets/attention.png" width="80%" align="middle" class="svelte-126cykp"/></div> <div class="figure-caption svelte-126cykp">Figure <span class="attention svelte-126cykp">X</span>. Using Query, Key, and Value matrices to
						calculate masked self-attention.</div> <ul class="svelte-126cykp"><li class="svelte-126cykp"><strong>Attention Score</strong>: The dot product of
							<span class="q-color svelte-126cykp">Query</span>
							and <span class="k-color svelte-126cykp">Key</span> matrices determines the alignment of each query with
							each key, producing a square matrix that reflects the relationship between all input tokens.</li> <li class="svelte-126cykp"><strong>Masking</strong>: A mask is applied to the upper triangle of the attention
							matrix to prevent the model from accessing future tokens, setting these values to
							negative infinity. The model needs to learn how to predict the next token without
							“peeking” into the future.</li> <li class="svelte-126cykp"><strong>Softmax</strong>: After masking, the attention score is converted into
							probability by the softmax operation which takes the exponent of each attention score.
							Each row of the matrix sums up to one and indicates the relevance of every other token
							to the left of it.</li></ul></div> <div class="article-subsection-l2 svelte-126cykp"><h4 class="svelte-126cykp">Step 3: Output</h4> <p class="svelte-126cykp">The model uses the masked self-attention scores and multiplies them with the
						<span class="v-color svelte-126cykp">Value</span> matrix to get the
						<span class="purple-color svelte-126cykp">final output</span>
						of the self-attention mechanism. GPT-2 has <code class="svelte-126cykp">12</code> self-attention heads, each capturing
						different relationships between tokens. The outputs of these heads are concatenated and passed
						through a linear projection.</p></div> <div class="article-subsection svelte-126cykp"><h3 class="svelte-126cykp">MLP: Multi-Layer Perceptron</h3> <div class="figure svelte-126cykp"><img src="./article_assets/mlp.png" width="70%" align="middle" class="svelte-126cykp"/></div> <div class="figure-caption svelte-126cykp">Figure <span class="attention svelte-126cykp">X</span>. Using MLP layer to project the self-attention
						representations into higher dimensions to enhance the model&#39;s representational capacity.</div> <p class="svelte-126cykp">After the multiple heads of self-attention capture the diverse relationships between the
						input tokens, the concatenated outputs are passed through the Multilayer Perceptron
						(MLP) layer to enhance the model&#39;s representational capacity. The MLP block consists of
						two linear transformations with a GELU activation function in between. The first linear
						transformation increases the dimensionality of the input four-fold from <code class="svelte-126cykp">768</code>
						to <code class="svelte-126cykp">3072</code>. The second linear transformation reduces the dimensionality back
						to the original size of <code class="svelte-126cykp">768</code>, ensuring that the subsequent layers receive
						inputs of consistent dimensions. Unlile the self-attention mechanis, the MLP processes
						tokens independently and simply maps them from one representation to another.</p></div> <div class="architecture-section svelte-126cykp"><h2 class="svelte-126cykp">Output Probabilities</h2> <p class="svelte-126cykp">After the input has been processed through all Transformer blocks, the output is passed
						through the final linear layer to prepare it for token prediction. This layer projects
						the final representations into a <code class="svelte-126cykp">50,257</code>
						dimensional space, where every token in the vocabulary has a corresponding value called
						<code class="svelte-126cykp">logit</code>. Any token can be the next word, so this process allows us to simply
						rank these tokens by their likelihood of being that next word. We then apply the softmax
						function to convert the logits into a probability distribution that sums to one. This
						will allow us to sample the next token based on its likelihood.</p> <div class="figure svelte-126cykp"><img src="./article_assets/softmax.png" width="60%" align="middle" class="svelte-126cykp"/></div> <div class="figure-caption svelte-126cykp">Figure <span class="attention svelte-126cykp">X</span>. Each token in the vocabulary is assigned a
						probability based on the model&#39;s output logits. These probabilities determine the
						likelihood of each token being the next word in the sequence.</div> <p class="svelte-126cykp">The final step is to generate the next token by sampling from this distribution The <code class="svelte-126cykp">temperature</code>
						hyperparameter plays a critical role in this process. Mathematically speaking, it is a very
						simple operation: model output logits are simply divided by the
						<code class="svelte-126cykp">temperature</code>:</p> <ul class="svelte-126cykp"><li class="svelte-126cykp"><code class="svelte-126cykp">temperature = 1</code>: Dividing logits by one has no effect on the softmax
							outputs.</li> <li class="svelte-126cykp"><code class="svelte-126cykp">temperature &lt; 1</code>: Lower temperature makes the model more confident and
							deterministic by sharpening the probability distribution, leading to more predictable
							outputs.</li> <li class="svelte-126cykp"><code class="svelte-126cykp">temperature &gt; 1</code>: Higher temperature creates a softer probability
							distribution, allowing for more randomness in the generated text – what some refer to
							as model <em>“creativity”</em>.</li></ul> <p class="svelte-126cykp">Adjust the temperature and see how you can balance between deterministic and diverse
						outputs!</p></div> <div class="architecture-section svelte-126cykp"><h2 class="svelte-126cykp">Advanced Architectural Features</h2> <p class="svelte-126cykp">There are several advanced architectural features that enhance the performance of
						Transformer models. While important for the model&#39;s overall performance, they are not as
						important for understanding the core concepts of the architecture. Layer Normalization,
						Dropout, and Residual Connections are crucial components in Transformer models,
						particularly during the training phase. Layer Normalization stabilizes training and
						helps the model converge faster. Dropout prevents overfitting by randomly deactivating
						neurons. Residual Connections allows gradients to flow directly through the network and
						helps to prevent the vanishing gradient problem.</p> <div class="article-subsection svelte-126cykp"><h3 class="svelte-126cykp">Layer Normalization</h3> <p class="svelte-126cykp">Layer Normalization helps to stabilize the training process and improves convergence.
							It works by normalizing the inputs across the features, ensuring that the mean and
							variance of the activations are consistent. This normalization helps mitigate issues
							related to internal covariate shift, allowing the model to learn more effectively and
							reducing the sensitivity to the initial weights. Layer Norm is applied twice in each
							Transformer block, once before the self-attention mechanism and once before the MLP
							layer.</p></div> <div class="article-subsection svelte-126cykp"><h3 class="svelte-126cykp">Dropout</h3> <p class="svelte-126cykp">Dropout is a regularization technique used to prevent overfitting in neural networks
							by randomly setting a fraction of model wights to zero during training. This
							encourages the model to learn more robust features and reduces dependency on specific
							neurons, helping the network generalize better to new, unseen data. During model
							inference, dropout is deactivated. This essentially means that we are using an
							ensemble of the trained subnetworks, which leads to a better model performance.</p></div> <div class="article-subsection svelte-126cykp"><h3 class="svelte-126cykp">Residual Connections</h3> <p class="svelte-126cykp">Residual connections were first introduced in the ResNet model in 2015. This
							architectural innovation revolutionized deep learning by enabling the training of very
							deep neural networks. Essentially, residual connections are shortcuts that bypass one
							or more layers, adding the input of a layer to its output. This helps mitigate the
							vanishing gradient problem, making it easier to train deep networks with multiple
							Transformer blocks stacked on top of each other. In GPT-2, residual connections are
							used twice within each Transformer block: once before the MLP and once after, ensuring
							that gradients flow more easily, and earlier layers receive sufficient updates during
							backpropagation.</p></div></div> <div class="article-section svelte-126cykp"><h1 class="svelte-126cykp">Interactive Features</h1> <p class="svelte-126cykp">Transformer Explainer is built to be interactive and allow you to explore the inner
						workings of the Transformer. Here are some of the interactive features you can play
						with:</p> <ul class="svelte-126cykp"><li class="svelte-126cykp"><strong>Input your own text sequence</strong> to see how the model processes it and predicts
							the next word. Explore attention weights, intermediate computations, and see how the final
							output probabilities are calculated.</li> <li class="svelte-126cykp"><strong>Use the temperature slider</strong> to control the randomness of the model’s predictions.
							Explore how you can make the model output more deterministic or more creative by changing
							the temperature value.</li> <li class="svelte-126cykp"><strong>Interact with attention maps</strong> to see how the model focuses on different
							tokens in the input sequence. Hover over tokens to highlight their attention weights and
							explore how the model captures context and relationships between words.</li></ul></div> <div class="article-section svelte-126cykp"><h2 class="svelte-126cykp">Video Tutorial</h2> <span class="attention svelte-126cykp">FUTURE_LINK_TO_YOUTUBE_TUTORIAL</span></div> <div class="article-section svelte-126cykp"><h2 class="svelte-126cykp">How is Transformer Explainer implemented?</h2> <p class="svelte-126cykp">Transformer Explainer features a live GPT-2 (small) model running directly in the
						browser. This model is derived from the PyTorch implementation of GPT by Andrej
						Karpathy&#39;s
						<a href="https://github.com/karpathy/nanoGPT" title="Github" target="_blank" class="svelte-126cykp">nanoGPT project</a>
						and has been converted to
						<a href="https://onnxruntime.ai/" title="ONNX" target="_blank" class="svelte-126cykp">ONNX Runtime</a>
						for seamless in-browser execution. The interface is built using JavaScript, with
						<a href="https://kit.svelte.dev/" title="Svelte" target="_blank" class="svelte-126cykp">Svelte</a>
						as a front-end framework and
						<a href="http://D3.js" title="D3" target="_blank" class="svelte-126cykp">D3.js</a>
						for creating dynamic visualizations. Numerical values are updated live following the user
						input.</p></div> <div class="article-section svelte-126cykp"><h2 class="svelte-126cykp">Who developed the Transformer Explainer?</h2> <p class="svelte-126cykp">Transformer Explainer was created by

						<a href="https://aereeeee.github.io/" target="_blank" class="svelte-126cykp">Aeree Cho</a>,
						<a href="https://www.linkedin.com/in/chaeyeonggracekim/" target="_blank" class="svelte-126cykp">Grace C. Kim</a>,
						<a href="https://alexkarpekov.com/" target="_blank" class="svelte-126cykp">Alexander Karpekov</a>,
						<a href="https://alechelbling.com/" target="_blank" class="svelte-126cykp">Alec Helbling</a>,
						<a href="https://zijie.wang/" target="_blank" class="svelte-126cykp">Jay Wang</a>,
						<a href="https://seongmin.xyz/" target="_blank" class="svelte-126cykp">Seongmin Lee</a>,
						<a href="https://bhoov.com/" target="_blank" class="svelte-126cykp">Benjamin Hoover</a>, and
						<a href="https://poloclub.github.io/polochau/" target="_blank" class="svelte-126cykp">Polo Chau</a>

						at the Georgia Institute of Technology.</p></div></div></div></div>`;return{c(){e=D("div"),e.innerHTML=t,this.h()},l(n){e=I(n,"DIV",{id:!0,class:!0,"data-svelte-h":!0}),re(e)!=="svelte-1mgf6vj"&&(e.innerHTML=t),this.h()},h(){m(e,"id","description"),m(e,"class","svelte-126cykp")},m(n,s){M(n,e,s)},p:Y,i:Y,o:Y,d(n){n&&p(e)}}}function ss(l){return ct(ut),[]}class ls extends J{constructor(e){super(),x(this,e,ss,ts,G,{})}}function as(l){let e,t,n,s,a,i,u,o,r,c,d,h,k;a=new es({});const v=l[3].default,b=le(v,l,l[2],null);return h=new ls({}),{c(){e=U(),t=D("div"),n=D("div"),s=D("header"),K(a.$$.fragment),u=U(),o=D("main"),b&&b.c(),c=U(),d=D("div"),K(h.$$.fragment),this.h()},l(y){xe("svelte-elip4h",document.head).forEach(p),e=F(y),t=I(y,"DIV",{id:!0,style:!0,class:!0});var q=E(t);n=I(q,"DIV",{id:!0,class:!0});var z=E(n);s=I(z,"HEADER",{class:!0});var H=E(s);R(a.$$.fragment,H),H.forEach(p),u=F(z),o=I(z,"MAIN",{id:!0,style:!0,class:!0});var N=E(o);b&&b.l(N),N.forEach(p),z.forEach(p),c=F(q),d=I(q,"DIV",{class:!0});var g=E(d);R(h.$$.fragment,g),g.forEach(p),q.forEach(p),this.h()},h(){document.title="Transformer Explainer",m(s,"class","svelte-ptfu7x"),$e(()=>l[4].call(s)),m(o,"id","main"),m(o,"style",r=`padding-top:${l[0]}px`),m(o,"class","svelte-ptfu7x"),m(n,"id","landing"),m(n,"class","svelte-ptfu7x"),m(d,"class","article h-auto w-full"),m(t,"id","app"),m(t,"style",`--min-screen-width:${Oe}px;--min-column-width:${l[1]}px;--predicted-color:${dt};`),m(t,"class","svelte-ptfu7x")},m(y,T){M(y,e,T),M(y,t,T),w(t,n),w(n,s),O(a,s,null),i=et(s,l[4].bind(s)),w(n,u),w(n,o),b&&b.m(o,null),w(t,c),w(t,d),O(h,d,null),k=!0},p(y,[T]){b&&b.p&&(!k||T&4)&&ae(b,v,y,y[2],k?ie(v,y[2],T,null):ne(y[2]),null),(!k||T&1&&r!==(r=`padding-top:${y[0]}px`))&&m(o,"style",r)},i(y){k||(P(a.$$.fragment,y),P(b,y),P(h.$$.fragment,y),k=!0)},o(y){V(a.$$.fragment,y),V(b,y),V(h.$$.fragment,y),k=!1},d(y){y&&(p(e),p(t)),Q(a),i(),b&&b.d(y),Q(h)}}}let Oe=1400;function ns(l,e,t){let{$$slots:n={},$$scope:s}=e,a=0,i=Math.floor(Oe/24)-ht*2;function u(){a=this.offsetHeight,t(0,a)}return l.$$set=o=>{"$$scope"in o&&t(2,s=o.$$scope)},[a,i,s,n,u]}class hs extends J{constructor(e){super(),x(this,e,ns,as,G,{})}}export{hs as component};
